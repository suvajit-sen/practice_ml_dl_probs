{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries up front\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file for Q&A\n",
    "with open('train_qa.txt', 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test file also\n",
    "with open('test_qa.txt', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Now visualize the data : how the data has been stored\n",
    "print(type(train_data))\n",
    "print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will create a vocabulary of all the words\n",
    "all_data = test_data + train_data\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'garden', 'hallway', 'picked', 'apple', 'office', 'bedroom', 'put', 'discarded', 'Is', 'bathroom', 'Mary', 'Sandra', 'got', 'back', 'travelled', 'to', '.', 'football', 'up', 'Daniel', 'the', 'down', 'journeyed', 'dropped', 'moved', 'grabbed', 'kitchen', 'there', 'milk', 'left', 'John', 'yes', 'took', 'in', 'went', 'no', '?'}\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# Now we will create a set of vocab, coz set will never take duplicate values\n",
    "#\n",
    "vocab = set()\n",
    "\n",
    "for story, question, answer in all_data:\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))\n",
    "\n",
    "# add yes and no also\n",
    "vocab.add('yes')\n",
    "vocab.add('no')\n",
    "\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 6\n"
     ]
    }
   ],
   "source": [
    "# now we have to know the length of the vocab and the max story length which will be required in Padding\n",
    "all_story_len = [len(data[0]) for data in all_data]\n",
    "#print(all_story_len)\n",
    "max_story_len = max(all_story_len)\n",
    "\n",
    "# length of longest question\n",
    "all_quest_len = [len(data[1]) for data in all_data]\n",
    "#print(all_story_len)\n",
    "max_quest_len = max(all_quest_len)\n",
    "print(max_story_len, max_quest_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "G:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "G:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "G:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "G:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "G:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "G:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Now we will vectorize the data with the help of Keras \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'garden': 1,\n",
       " 'hallway': 2,\n",
       " 'picked': 3,\n",
       " 'apple': 4,\n",
       " 'office': 5,\n",
       " 'bedroom': 6,\n",
       " 'put': 7,\n",
       " 'discarded': 8,\n",
       " 'is': 9,\n",
       " 'bathroom': 10,\n",
       " 'mary': 11,\n",
       " 'sandra': 12,\n",
       " 'got': 13,\n",
       " 'back': 14,\n",
       " 'travelled': 15,\n",
       " 'to': 16,\n",
       " '.': 17,\n",
       " 'football': 18,\n",
       " 'up': 19,\n",
       " 'daniel': 20,\n",
       " 'the': 21,\n",
       " 'down': 22,\n",
       " 'journeyed': 23,\n",
       " 'dropped': 24,\n",
       " 'moved': 25,\n",
       " 'grabbed': 26,\n",
       " 'kitchen': 27,\n",
       " 'there': 28,\n",
       " 'milk': 29,\n",
       " 'left': 30,\n",
       " 'john': 31,\n",
       " 'yes': 32,\n",
       " 'took': 33,\n",
       " 'in': 34,\n",
       " 'went': 35,\n",
       " 'no': 36,\n",
       " '?': 37}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tokenizer and fit it with our vocabulary\n",
    "tokenizer = Tokenizer(filters= [])\n",
    "tokenizer.fit_on_texts(vocab) # this is where we are fitting the tokenizer with our own vocabulary\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.']\n",
      "['Is', 'Sandra', 'in', 'the', 'hallway', '?']\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "# Now we have to tokenize the story, questions and answers\n",
    "train_story_text = []\n",
    "train_quest_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story, question, answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_quest_text.append(question)\n",
    "    train_answers.append(answer)\n",
    "\n",
    "print(train_story_text[0])\n",
    "print(train_quest_text[0])\n",
    "print(train_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 25, 16, 21, 10, 17, 12, 23, 16, 21, 6, 17]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to convert this text to numbers/ sequences\n",
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)\n",
    "print(len(train_story_seq))\n",
    "train_story_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to vectorize the stories and questions. Padding also is to be done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
